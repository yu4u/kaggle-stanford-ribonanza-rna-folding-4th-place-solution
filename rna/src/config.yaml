task:  # task specific config
  mode: "train"  # pretrain, train
  pseudo: false
  oof: false
  sn_th: 1.0
  ngram: 1

model:
  arch: transformer  # transformer
  resume_path: null
  num_layers: 12
  hidden_dim: 192
  nhead: 6
  drop_path_rate: 0.2
  norm_first: true
  ema: false
  swa: false
  freeze_backbone: false
  freeze_end_epoch: 16

data:
  fold_num: 5
  fold_id: 0
  num_workers: 0
  batch_size: 128
  train_all: false

trainer:
  max_epochs: 128
  devices: "auto"  # list or str, -1 to indicate all available devices
  strategy: "auto"  # ddp
  check_val_every_n_epoch: 1
  sync_batchnorm: false
  accelerator: "cpu"  # cpu, gpu, tpu, ipu, hpu, mps, auto
  precision: 32  # 16, 32, 64, bf16
  gradient_clip_val: null
  accumulate_grad_batches: 1
  deterministic: true

test:
  mode: test  # test or val
  output_dir: preds_results

opt:
  opt: "AdamW"  # SGD, Adam, AdamW, ...
  lr: 2e-3
  weight_decay: 0.05

scheduler:
  sched: "cosine"
  min_lr: 2e-4
  warmup_epochs: 0

wandb:
  project: rna-main
  name: null
  fast_dev_run: false
